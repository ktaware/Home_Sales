{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "outputId": "3f9cf0f1-daca-44e1-82df-22d7d1c9d435"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop3.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.3.2-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\uoft\\lib\\site-packages\\findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_python\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpy4j-*.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Start a SparkSession\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mfindspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uoft\\lib\\site-packages\\findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    163\u001b[0m                 spark_python\n\u001b[0;32m    164\u001b[0m             )\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.3.2-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.3.1'\n",
    "spark_version = 'spark-3.3.2'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krBMDaIICaeY"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XbWNf1Te5fM"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "1cf8bb9c-6476-4037-b3c7-b9a0c351b058"
   },
   "outputs": [],
   "source": [
    "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "homes_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True, ignoreLeadingWhiteSpace=True) #Observe the need to use ignoreLeadingWhiteSpace=True, otherwise a leading whitespace will appear in the column names\n",
    "homes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoljcJ7WPpnm"
   },
   "outputs": [],
   "source": [
    "# 2. Create a temporary view of the DataFrame.\n",
    "\n",
    "homes_df.createOrReplaceTempView('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "7c329c80-5514-4e4a-c73b-cf8c7dfac84b"
   },
   "outputs": [],
   "source": [
    "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "\n",
    "a = \"\"\"\n",
    "SELECT\n",
    "  YEAR(date) AS YEAR,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 4\n",
    "GROUP BY YEAR\n",
    "ORDER BY YEAR DESC\n",
    "\"\"\"\n",
    "spark.sql(a).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "1afbf796-6901-4193-d4bd-f57f59bbdcff"
   },
   "outputs": [],
   "source": [
    "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
    "\n",
    "b =  \"\"\"\n",
    "SELECT\n",
    "  YEAR(date_built) AS YEAR,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 3\n",
    "and bathrooms = 3\n",
    "GROUP BY YEAR(date_built)\n",
    "ORDER BY YEAR DESC\n",
    "\"\"\"\n",
    "spark.sql(b).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "17a4b618-196c-4075-f5c6-01b88679837b"
   },
   "outputs": [],
   "source": [
    "# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "\n",
    "c = \"\"\"\n",
    "SELECT\n",
    "  YEAR(date_built) AS YEAR_BUILT,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 3\n",
    "and bathrooms = 3\n",
    "and sqft_living >= 2000\n",
    "and floors = 2\n",
    "GROUP BY YEAR_BUILT\n",
    "ORDER BY YEAR_BUILT DESC\n",
    "\"\"\"\n",
    "spark.sql(c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "61331ad6-a19d-40f3-deb3-d7015ad45abd"
   },
   "outputs": [],
   "source": [
    "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
    "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "\n",
    "start_time = time.time()\n",
    "d = \"\"\"\n",
    "SELECT\n",
    "  view, \n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "GROUP BY view\n",
    "HAVING AVG(price) >= 350000\n",
    "ORDER BY view desc\n",
    "\"\"\"\n",
    "spark.sql(d).show()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "abe00d59-0cd3-466d-a416-7d3cc51f69ef"
   },
   "outputs": [],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "spark.sql(\"cache table home_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "58988d6a-7996-42a3-b6f7-0f1adbe3091f"
   },
   "outputs": [],
   "source": [
    "# 8. Check if the table is cached.\n",
    "spark.catalog.isCached('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "37df2347-d85f-4e11-c937-c259c145c6f6"
   },
   "outputs": [],
   "source": [
    "# 9. Using the cached data, run the query that filters out the view ratings with average price \n",
    "#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "e = \"\"\"\n",
    "SELECT\n",
    "  view,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "GROUP BY view\n",
    "HAVING AVG(price) >= 350000\n",
    "ORDER BY view desc\n",
    "\"\"\"\n",
    "spark.sql(e).show()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [],
   "source": [
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "homes_df.write.partitionBy('date_built').parquet('p_home_sales',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [],
   "source": [
    "# 11. Read the parquet formatted data.\n",
    "p_homes_df = spark.read.parquet('p_home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [],
   "source": [
    "# 12. Create a temporary table for the parquet data.\n",
    "p_homes_df.createOrReplaceTempView('p_homesales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "993e7461-5027-4074-afe7-2a8b68148647"
   },
   "outputs": [],
   "source": [
    "# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000 \n",
    "# with the parquet DataFrame. Round your average to two decimal places. \n",
    "# Determine the runtime and compare it to the cached version.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "e = \"\"\"\n",
    "SELECT\n",
    "  view,\n",
    "  ROUND(AVG(price), 2) AS AVERAGE_PRICE\n",
    "FROM home_sales\n",
    "GROUP BY view\n",
    "HAVING AVG(price) >= 350000\n",
    "ORDER BY view desc\n",
    "\"\"\"\n",
    "spark.sql(e).show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "da2b9b24-a842-43dd-ab40-759f2fee9cbf"
   },
   "outputs": [],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n",
    "spark.sql(\"uncache table home_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "eae83c58-3471-4075-e6c1-f8374cb226bb"
   },
   "outputs": [],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "\n",
    "if spark.catalog.isCached('home_sales'):\n",
    "  print('home_sales is cached')\n",
    "else:\n",
    "  print('not cached')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
